\documentclass[final,leqno]{siamltex1213}

\usepackage{fontspec}
\usepackage{microtype}

\usepackage{newunicodechar}
\newunicodechar{α}{\ensuremath \alpha}
\newunicodechar{β}{\ensuremath \beta}

\usepackage{listings}
\lstdefinelanguage{julia}{
  basicstyle=\small\ttfamily,
  showspaces=false,
  showstringspaces=false,
  keywordstyle={\textbf},
  morekeywords={if,else,elseif,while,for,begin,end,quote,try,catch,return,local,abstract,function,generated,macro,ccall,finally,typealias,break,continue,type,global,module,using,import,export,const,let,bitstype,do,in,baremodule,importall,immutable},
  escapeinside={~}{~},
  morecomment=[l]{\#},
  commentstyle={},
  morestring=[b]",
}
\lstset{language=julia, numbers=left, numberstyle=\tiny, mathescape=true}

\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algorithmic}
\bibliographystyle{siam}

\title{Practical Lanczos bidiagonalizations in Julia
    \thanks{This
        work was supported by the
	%\todo{ALAN to supply}
	}}

\author{%
    Jiahao Chen
    \thanks{Computer Science and Artificial Intelligence Laboratory,
           Massachusetts Institute of Technology,
           Cambridge, Massachusetts, 02139 ({\tt jiahao@mit.edu})}
    %
    \and
    Andreas Noack
    \thanks{Computer Science and Artificial Intelligence Laboratory,
            Massachusetts Institute of Technology,
            Cambridge, Massachusetts, 02139 ({\tt noack@mit.edu})}
    %
    \and
    Alan Edelman
    \thanks{Department of Mathematics and Computer Science and Artificial Intelligence Laboratory,
            Massachusetts Institute of Technology,
            Cambridge, Massachusetts, 02139 ({\tt edelman@mit.edu})}
}


\begin{document}

\maketitle

\begin{abstract}
We describe the implementation of a practical Krylov-Schur
\end{abstract}

\begin{keywords}
\end{keywords}

\begin{AMS}
\end{AMS}

\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{J. CHEN, A. NOACK AND A. S. EDELMAN}{Practical SVD in Julia}

\listoftodos

This paper is to describe the role that partial factorizations play in
organizing the various methods for doing singular value decompositions in
Julia


\section{Introduction}

The singular value decomposition (SVD) of a $m\times n$ matrix is
the product of matrices
\[
A=USV^{T}
\]
where $S$ is a diagonal real matrix, ...

SVDs are popular for tint many contests itncluding the princiapal
components anlayss bkah blah blaha dn it si salso known as the Kahunen-Loeve
decompositoin in other conetxts.

Of particular interest in the use of low rank approximations for SVDS.
In this case it is not necessary to compute $U$, $S$ and $V$ and
their entirety; rather, it suffices to determine the first $k$ diagonal
entries of $S$, being the $k$ largest singular values, and their
corresponding columns of $U$ and $V$.

The Lanczos bidiagonalization~\cite{Golub1965} is a natural choice.

This paper describes an implementation of the Lanczos bidiagonalization
method written in pure Julia.


\section{Implementing simple bidiagonalization in Julia}

\begin{algorithm}
\caption{Simple Golub-Kahan-Lanczos bidiagonalization in pseudocode}

\begin{algorithmic}
\REQUIRE A matrix $A$ and a unit vector $q$
\STATE $\beta_0$ = 0
\FOR{$j=1,2,\dots,k$}

\STATE $\tilde{p}_j = A q_j - \beta_{j-1} p_{j-1}$
\STATE $\alpha_j = \left\Vert \tilde{p}_j \right\Vert_2$
\STATE $p_j = \tilde{p}_j / \alpha_j$

\STATE $\tilde{q}_{j+1} = A^\prime p_j - \alpha_j q_j$
\STATE $\beta_j = \left\Vert \tilde{q}_{j+1} \right\Vert_2$
\STATE $q_{j+1} = \tilde{q}_{j+1} / \beta_j$

\ENDFOR
\RETURN $\left(p_1, \dots, p_k, q_1, \dots, q_{k+1},\right) \alpha_1, \dots, \alpha_{k+1}, \beta_1, \dots, \beta_{k+1}$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Simple Golub-Kahan-Lanczos bidiagonalization in Julia}

\begin{lstlisting}
function svd_gkl(A, q; maxiter=minimum(size(A)))
    m, n = size(A)
    T = eltype(A)
    Tr = typeof(one(T)/norm([one(T)]))

    $α$s = Tr[]
    $β$s = Tr[]
    P = zeros(T, m, 0)
    Q = [q]

    $β$ = zero(Tr)
    p = zeros(size(A, 1))
    for iter in 1:maxiter
        p = A*q - $β$*p
	$α$ = norm(p)
        p = p/$α$
        push!($α$s, α)
        P = [P p]

        q = A'p - $α$*q
        $β$ = norm(q)
        q = q/$β$
        push!($β$s, β)
        Q = [Q q]
    end

    B = Bidiagonal($α$s, $β$s[1:maxiter-1], true)
    F = svdfact(B)
    LinAlg.SVD(P'F[:U], F[:S], F[:Vt]*Q[:,1:maxiter]')
end
\end{lstlisting}
\end{algorithm}

Julia is a high level dynamic language for technical
computing~\cite{Bezanson2012,Bezanson2015}. While it shares superficial
syntactic similarities with other high level languages such as Matlab or
Python, Julia was designed specifically for technical computing for the ground
up, with a syntax that supports the expressiveness demanded by mathematical
notation while facilitating compiler optimizations to generate fast machine
code.

Let's explain some feature of Julia exhibited by this code listing.

\paragraph{Type arithmetic.}
This function takes as input any $A$ that supports
matrix-vector products of the form $Aq$ and $A^\prime p$. As such, the element
types of $U$, $S$ and $V$ depend on the element type of $A$.

\paragraph{Special matrix types.}
Julia's base library also supports many special matrix types, such as
\verb|Bidiagonal| for bidiagonal matrices. The \verb|Bidiagonal| constructor
seen in this code listing creates an object that represents a bidiagonal
matrix, but stores only the diagonal and its superdiagonal (the \verb|true|
argument denotes that the matrix is upper bidiagonal). The generic function
system available in Julia~\cite{Bezanson2012,Bezanson2015} allows us to define
multimethods for functions such as \verb|svdfact| for computing the SVD
factorization of a matrix, but without restricting the function to work only
on ordinary dense matrices. In fact, it is precisely the definition of a
\verb|Bidiagonal| object (called a \verb|type| in Julia) which allows us to
specify which algorithm to run.

A simplified definition of the \verb|svdfact| function in the base Julia
library looks like this:

\begin{lstlisting}
function svdfact{T<:BlasFloat}(A::StridedMatrix{T}; thin::Bool=true)
    U, S, V$^T$ = LAPACK.gesdd!(thin ? 'S' : 'A', copy(A))
    SVD(U, S, V$^T$)
end

function svdfact{T<:BlasFloat}(A::Bidiagonal{T})
    D, _, U, Vt, _, _ = LAPACK.bdsdc!(M.isupper ? 'U' : 'L',
                        'I', copy(A.dv), copy(A.ev))
    SVD(U, S, V$^T$)
end
\end{lstlisting}

The \verb|svdfact| function has two methods defined. The first defines a SVD
factorization on dense matrices as the result of LAPACK's \verb|_GESDD|
function, wrapping the output in an \verb|SVD| object. The \verb|SVD| object itself is defined as

\begin{lstlisting}
immutable SVD{T,Tr,M<:AbstractArray} <: Factorization{T}
    U::M
    S::Vector{Tr}
    V$^T$::M
end
\end{lstlisting}


annd the other on \verb|Bidiagonal| matrices. berbidiaogonal thus we
sw



\paragraph{Special factorization types.}
Low cost abstractions


\section{Practical considerations}


\subsection{Reorthogonalization}

\missingfigure{Code listing for reorthogonalizations}

\todo{Jiahao}

\subsection{Krylov-Schur restarting}

\missingfigure{Code listing for thick restart}

\todo{Jiahao}

\subsection{Error analysis}

\todo{Jiahao: Derive correct residual}


\section{``Applications of overloading''}

\subsection{Generalization to Block Lanczos}

\todo{Andreas}

\subsection{Generalization to quaternions and BigFloats}

\todo{Andreas}

\subsection{Distributed linear algebra}

\paragraph{Overloading 5 functions gets you ScaLAPACK/Elemental integration.}

\todo{Andreas and Jake}



\section{Results}

Pick some specific examples

Pick some convenient Matrix Market matrix which produces some acceleration of
the problem by monitoring the error bounds on individual matrices vs the entire
residual norm.

\missingfigure{Convergence as a function of error bars on singular values vs residuals}

\todo{Jake}


\subsection{Serial linear algebra}

\missingfigure{Execution time of Julia vs ARPACK vs PROPACK vs SLEPc on one core}


\todo{Andreas and Jake and Jiahao}


\subsection{Comparison with randomized algorithms}

Comparison with flashpca or randomized subspace interaction?

Show error bound computations.

\todo{Jiahao}



\subsection{Distributed linear algebra}

\missingfigure{Strong scaling for parallel linalg on some problem}

\missingfigure{Weak scaling for parallel linalg on some problem}

Julia/ScaLAPACK vs Julia/Elemental vs Julia/MKL vs SLEPc vs PARPACK vs Parallel PROPACK (?)


\todo{Andreas and Jake}

\section{Conclusions}



\section*{Acknowledgments}

We thank the Julia development community for their contributions to free and
open source software. Julia is free software that can be downloaded from
\url{julialang.org/downloads}. The implementation of iterative SVD described in
this paper is available as the \verb|svdl| function in the
\url[IterativeSolvers.jl]{https://github.com/JuliaLang/IterativeSolvers.jl}
package. J.C. would also like to thank Jack Poulson (Stanford), David Silvester
(Manchester), and Alex Townsend (MIT) for many insightful discussions.

\bibliography{svd}

\end{document}
