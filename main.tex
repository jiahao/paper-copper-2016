\documentclass[final,leqno]{siamltex1213}

\usepackage{fontspec}
\usepackage{microtype}

\usepackage{newunicodechar}
\newunicodechar{α}{\ensuremath \alpha}
\newunicodechar{β}{\ensuremath \beta}

\usepackage{amsmath}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}

\usepackage{listings}
\lstdefinelanguage{julia}{
  basicstyle=\small\ttfamily,
  showspaces=false,
  showstringspaces=false,
  keywordstyle={\textbf},
  morekeywords={if,else,elseif,while,for,begin,end,quote,try,catch,return,local,abstract,function,generated,macro,ccall,finally,typealias,break,continue,type,global,module,using,import,export,const,let,bitstype,do,in,baremodule,importall,immutable},
  escapeinside={~}{~},
  morecomment=[l]{\#},
  commentstyle={},
  morestring=[b]",
}
\lstset{language=julia, numbers=left, numberstyle=\tiny, mathescape=true}

\usepackage{todonotes}
\usepackage{algorithm}
\usepackage{algorithmic}
\bibliographystyle{siam}

\title{Practical Lanczos bidiagonalizations in Julia
    \thanks{This
        work was supported by the
	%\todo{ALAN to supply}
	}}

\author{%
    Jiahao Chen
    \thanks{Computer Science and Artificial Intelligence Laboratory,
           Massachusetts Institute of Technology,
           Cambridge, Massachusetts, 02139 ({\tt jiahao@mit.edu})}
    %
    \and
    Andreas Noack
    \thanks{Computer Science and Artificial Intelligence Laboratory,
            Massachusetts Institute of Technology,
            Cambridge, Massachusetts, 02139 ({\tt noack@mit.edu})}
}


\begin{document}

\maketitle

\begin{abstract}
We describe the implementation of a practical Krylov-Schur
\end{abstract}

\begin{keywords}
\end{keywords}

\begin{AMS}
\end{AMS}

\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{J. CHEN AND A. NOACK}{Practical SVD in Julia}

\listoftodos

This paper is to describe the role that partial factorizations play in
organizing the various methods for doing singular value decompositions in
Julia


\section{GWAS}
Genome wide association studies (GWAS) seek to associate gene variation with variations in outcome variables typically measuring a quantity related to an illness. The typical measurement for the gene variation in the genotype at different locations in the genome. Not all locations in the genome have variation in the genotype across humans so a GWAS only includes locations with variation. Locations in the genome with variation are called single nucleotide polymorphisms (SNPs) and often the explanatory data in a GWAS is simply called SNP data. The various outcome variables in a GWAS are typically called phenotypes so in short GWASs examine the relation between genotypes and phenotypes. Often in statistical studies when data is observed passively there can be doubt about the causal direction for the variables included in the analysis. However, it is generally accepted that genotypes cannot be caused by other factors included in the analysis and consequently the correlation between genotypes and phenotype could in theory be considered causal.

The genotype data is collected for a groups of individuals of varying sizes. Some studies have genotype data for hundreds of individuals but the price of sequencing genome data is rapidly declining so collection of thousands of individuals are already becoming common and it is expected that we will soon  have access to genotype data for millions of individuals.

The classical assumption in statistical studies is that the observations are randomly sampled with replacement. The reason for assumption is the implication which is that the observations can be considered independent. This might not be exactly true but should be approximately true for many statistical results to be reliable. Most collections of SNP data are from patients at a single or few hospitals in a single country. The random sampling assumption is therefore often challenged

\subsection{The model}
In a GWAS, it is customary to use a regression model for associating the genotype variation at some location in the genome and the phenotype variation. The \emph{linear} regression model can be motivated either in several different way. The readers of this will probably be familiar with the least squares formulation of the regression model which simply seeks to find the coefficients that minimize the sum of squared distances between a hyperplane and the observations. Another familiar formulation is the projection formulation where the regression problem is seen as the problem of projection a vector of observations down on the space spanned by the vectors of explanatory variables.

Statisticians are interested in quantifying the uncertainty in quantifications and consequently they introduce the regression model from probabilistic in a probabilistic setting. Statisticians will assume that the effect from genotype to phenotype can be summarized by a conditional expectation. If we for individual $i$ denote the genotype measurement by $x_i$ and the phenotype measurement by $y_i$ this can be written as $\E(y_i|x_i)=\beta_0 + \beta_1 x_i$. A popular formulation of the this model is
\begin{equation*}
    y_i = \beta_0 + x_i \beta_1 + \varepsilon_i\quad i=1,\dots,n
\end{equation*}
where $n$ is the number of observations which in this case would be the number of individuals for which we have genotype data. The variable $\varepsilon_i$ is called the error term and must satisfy $\E(\varepsilon_i|x_i)=0$. More generally, $\varepsilon_i$ is the conditional distribution $y_i - \beta_0 - x_i\beta_1|x_i$. This can also be written in matrix form

\begin{equation}
\label{eq:linreg}
    y = X\beta + \varepsilon
\end{equation}

with
\begin{equation*}
    X = \begin{pmatrix}
        1 & x_1\\
        \vdots & \vdots \\
        1 & x_n
    \end{pmatrix}
\end{equation*}

and in this notation, the well known least squares estimator can be written as
\begin{equation}
\label{eq:ols}
    \hat{\beta} = (X'X)^{-1}X'y.
\end{equation}

It might seem unnecessarily complicated to use probability theory to derive the least squares solution to the regression problem. However, when $(x_i,y_i)$ pairs are considered to be random variables then also $\hat{\beta}$ is a random variable and the mean and variance of $\hat{\beta}$ can be used to quantify uncertainty about the least square solution.

First, notice that the least squares estimator \eqref{eq:ols} can be written
\begin{equation*}
    \hat{\beta} = \beta + (X'X)^{-1}X'\varepsilon
\end{equation*}
and the expected value of the estimator is
\begin{equation}
    \E(\hat{\beta}|X) = \E(\beta + (X'X)^{-1}X'\varepsilon|X) =
        \beta + (X'X)^{-1}X'\E(\varepsilon|X) = \beta
\end{equation}
which means that the estimator is \emph{unbiased}. Statisticians are interested in the variability of $\hat{\beta}$ under changes to the data than could be considered small errors and the most common measurement for the variability of an estimator is the (conditional) variance, i.e.
\begin{equation*}
    \Var(\hat{\beta}|X) = \Var(\beta + (X'X)^{-1}X'\varepsilon|X) =
        (X'X)^{-1}X'\Var(\varepsilon|X)X(X'X)^{-1}.
\end{equation*}

This shows that variance of $\hat{\beta}$ depends on the (conditional) variance of $\varepsilon$ which hasn't been discussed yet. In classical treatments of the linear regression model it is typically assumed that, conditionally on $x_i$, the $y_i$s are independent and the have the same variance which is the same as $\Var(\varepsilon|X)=\sigma^2 I$ for some unknown scalar $\sigma^2$. Under this assumption, the variance of $\hat{\beta}$ reduces to $\sigma^2 (X'X)^{-1}$. The magnitude of this quantity is unknown because $\sigma^2$ is an unknown parameter but $\sigma^2$ can be \emph{estimated} from the data. The usual estimator is $\hat{\sigma}^2=\frac{1}{n} \|\hat{\varepsilon}\|^2$ where $\hat{\varepsilon} = y - X\hat{\beta}$. This leads to the estimate of the (conditional) variance of the estimator $\widehat{\Var(\hat{\beta}|X)} = \hat{\sigma}^2(X'X)^{-1}$.

The independence assumption is often used in statistics and can be justified from an assumption of random sampling with replacement. In studies where data is passively collected, this might not be a reasonable assumption as explained in the previous section. Non-random sampling might lead to correlation between the phenotypes even after conditioning on the genotypes. In consequence of that, $\Var(\varepsilon|X)$ will no longer be diagonal but have some general positive definite structure $\Sigma$ and $\Var(\hat{\beta}|X)=(X'X)^{-1}X'\Sigma X(X'X)^{-1}$. Since $\Sigma$ in general is consists of $\frac{n(n+1)}{2}$ parameters it cannot be estimated consistently.

In order to analyze the problem with correlated observations, it is convenient to decompose the error into a part that contains the cross-individual correlation and a part that is diagonal and therefore only describes the variance for each individual. This may be written as

\begin{equation*}
    y_i = \beta_0 + x_i\beta_1 + \eta_i + \xi_i
\end{equation*}

where is assumed that $\Var(\eta|X)=\Sigma_\eta$ and $\Var(\xi|X)=\sigma^2_\xi I$. Furthermore, it is assumes that the two error terms are independent.

\subsection{Fixed effect estimation}
A possible solution to the problem of producing a reliable estimate of the variance of $\hat{\beta}$ is to come up with a set of variables $z_1,\dots,z_k$ that proxies the correlation between the observations, i.e. $\eta=Z\gamma$. By simply including the variables $z_1,\dots,z_k$ in the regression model, it possible to remove the correlation which distorts the variance estimate for $\hat{\beta}$. For the regression $y|X,Z$, we get the least squares estimator
\begin{equation*}
    \hat{\theta} = \begin{pmatrix}\hat{\beta} \\ \hat{\gamma}\end{pmatrix} =
    \begin{pmatrix}
        X'X & X'Z \\ Z'X & Z'Z
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        X' \\ Z'
    \end{pmatrix}
    (X\beta + Z\gamma + \xi)=
    \theta +
            \begin{pmatrix}
        X'X & X'Z \\ Z'X & Z'Z
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        X' \\ Z'
    \end{pmatrix}\xi.
\end{equation*}
which has variance
\begin{equation*}
    \Var(\hat{\theta}|X,Z)=\sigma_\xi^2
        \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1}
\end{equation*}.

In many applications, a few pricipal components of the covariance matrix of the complete SNP dataset is used to proxy for the correlation between individuals. Computing principal components is therefore often a first step in a GWAS. In the following section we give a brief overview over the available software for computing the principal component of SNP datasets.

\section{Software for computing the SVD of genomics data}

The software stack for GWAS is generally based on command line tools written completely in C/C++. Not only is the core computational algorithm written in C/C++ but also much of the pre- and postprocessing of the data. The datasets can be large but and the computations at times heavy but it has been a surprise to learn the extend to which analyses are carried out directly from the command line instead of using higher level languages like Matlab, R, or Python. This choice seems to limit the tools available to the analysts because, unless he is a C/C++ programmer, the programmer is restricted to the set of options included in the command line tool.

Two major packages exist for computing the PCA in the GWAS software stack. The package \emph{EIGENSOFT} accompanied~\cite{patterson2006population} which popularized the use of PCA in GWAS. In the original version of the package, the routine \texttt{smartpca} for computing the PCA of a SNP matrix was based on an eigenvalue solver from LAPACK. In consequence, all the eigenvalues and vectors of the SNP matrix were computed even though only a few of them were used as principal components.

More recently, \emph{FlashPCA}~\cite{abraham2014fast}, has emerged as a potentially faster alternative to EIGENSOFT's \texttt{smartpca}. The PCA routine is based on a truncated SVD algorithm described in~\cite{halko2011finding}. More specifically, FlashPCA uses a subspace iteration scheme with orthogonalization by QR in each iteration. The convergence criterion is based on the Frobenius norm of the changes to the orthogonal basis. The QR orthogonalization step in the implementation of FlashPCA routine deviates from the algorithm in described~\cite{abraham2014fast} which only normalizes columnwise. Our conjecture was that this change was made to avoid loss of orthogonality in the subspace basis and the author of the package has confirmed this to us. In consequence, the timings in~\cite{abraham2014fast} do not correspond to the performance of the software run with default settings since the QR orthogonalization is much slower than the columnwise normalization. Furthermore, a degenerate basis might also converge much faster because it eventually just converges to the single largest eigenvalue.

\section{Introduction}

The singular value decomposition (SVD) of a $m\times n$ matrix is
the product of matrices
\[
A=USV^{T}
\]
where $S$ is a diagonal real matrix, ...

SVDs are popular for tint many contests itncluding the princiapal
components anlayss bkah blah blaha dn it si salso known as the Kahunen-Loeve
decompositoin in other conetxts.

Of particular interest in the use of low rank approximations for SVDS.
In this case it is not necessary to compute $U$, $S$ and $V$ and
their entirety; rather, it suffices to determine the first $k$ diagonal
entries of $S$, being the $k$ largest singular values, and their
corresponding columns of $U$ and $V$.

The Lanczos bidiagonalization~\cite{Golub1965} is a natural choice.

This paper describes an implementation of the Lanczos bidiagonalization
method written in pure Julia.


\section{Implementing simple bidiagonalization in Julia}

\begin{algorithm}
\caption{Simple Golub-Kahan-Lanczos bidiagonalization in pseudocode}

\begin{algorithmic}
\REQUIRE A matrix $A$ and a unit vector $q$
\STATE $\beta_0$ = 0
\FOR{$j=1,2,\dots,k$}

\STATE $\tilde{p}_j = A q_j - \beta_{j-1} p_{j-1}$
\STATE $\alpha_j = \left\Vert \tilde{p}_j \right\Vert_2$
\STATE $p_j = \tilde{p}_j / \alpha_j$

\STATE $\tilde{q}_{j+1} = A^\prime p_j - \alpha_j q_j$
\STATE $\beta_j = \left\Vert \tilde{q}_{j+1} \right\Vert_2$
\STATE $q_{j+1} = \tilde{q}_{j+1} / \beta_j$

\ENDFOR
\RETURN $\left(p_1, \dots, p_k, q_1, \dots, q_{k+1},\right) \alpha_1, \dots, \alpha_{k+1}, \beta_1, \dots, \beta_{k+1}$
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Simple Golub-Kahan-Lanczos bidiagonalization in Julia}

\begin{lstlisting}
function svd_gkl(A, q; maxiter=minimum(size(A)))
    m, n = size(A)
    T = eltype(A)
    Tr = typeof(one(T)/norm([one(T)]))

    $α$s = Tr[]
    $β$s = Tr[]
    P = zeros(T, m, 0)
    Q = [q]

    $β$ = zero(Tr)
    p = zeros(size(A, 1))
    for iter in 1:maxiter
        p = A*q - $β$*p
	$α$ = norm(p)
        p = p/$α$
        push!($α$s, α)
        P = [P p]

        q = A'p - $α$*q
        $β$ = norm(q)
        q = q/$β$
        push!($β$s, β)
        Q = [Q q]
    end

    B = Bidiagonal($α$s, $β$s[1:maxiter-1], true)
    F = svdfact(B)
    LinAlg.SVD(P'F[:U], F[:S], F[:Vt]*Q[:,1:maxiter]')
end
\end{lstlisting}
\end{algorithm}

Julia is a high level dynamic language for technical
computing~\cite{Bezanson2012,Bezanson2015}. While it shares superficial
syntactic similarities with other high level languages such as Matlab or
Python, Julia was designed specifically for technical computing for the ground
up, with a syntax that supports the expressiveness demanded by mathematical
notation while facilitating compiler optimizations to generate fast machine
code.

Let's explain some feature of Julia exhibited by this code listing.

\paragraph{Type arithmetic.}
This function takes as input any $A$ that supports
matrix-vector products of the form $Aq$ and $A^\prime p$. As such, the element
types of $U$, $S$ and $V$ depend on the element type of $A$.

\paragraph{Special matrix types.}
Julia's base library also supports many special matrix types, such as
\verb|Bidiagonal| for bidiagonal matrices. The \verb|Bidiagonal| constructor
seen in this code listing creates an object that represents a bidiagonal
matrix, but stores only the diagonal and its superdiagonal (the \verb|true|
argument denotes that the matrix is upper bidiagonal). The generic function
system available in Julia~\cite{Bezanson2012,Bezanson2015} allows us to define
multimethods for functions such as \verb|svdfact| for computing the SVD
factorization of a matrix, but without restricting the function to work only
on ordinary dense matrices. In fact, it is precisely the definition of a
\verb|Bidiagonal| object (called a \verb|type| in Julia) which allows us to
specify which algorithm to run.

A simplified definition of the \verb|svdfact| function in the base Julia
library looks like this:

\begin{lstlisting}
function svdfact{T<:BlasFloat}(A::StridedMatrix{T}; thin::Bool=true)
    U, S, V$^T$ = LAPACK.gesdd!(thin ? 'S' : 'A', copy(A))
    SVD(U, S, V$^T$)
end

function svdfact{T<:BlasFloat}(A::Bidiagonal{T})
    D, _, U, Vt, _, _ = LAPACK.bdsdc!(M.isupper ? 'U' : 'L',
                        'I', copy(A.dv), copy(A.ev))
    SVD(U, S, V$^T$)
end
\end{lstlisting}

The \verb|svdfact| function has two methods defined. The first defines a SVD
factorization on dense matrices as the result of LAPACK's \verb|_GESDD|
function, wrapping the output in an \verb|SVD| object. The \verb|SVD| object itself is defined as

\begin{lstlisting}
immutable SVD{T,Tr,M<:AbstractArray} <: Factorization{T}
    U::M
    S::Vector{Tr}
    V$^T$::M
end
\end{lstlisting}


annd the other on \verb|Bidiagonal| matrices. berbidiaogonal thus we
sw



\paragraph{Special factorization types.}
Low cost abstractions


\section{Practical considerations}


\subsection{Partial reorthogonalization}

\missingfigure{Code listing for reorthogonalizations}

\todo{Jiahao}

\subsection{Thick restarting}

We also implement the thick restart methodo
\missingfigure{Code listing for thick restart}

\todo{Jiahao}

\subsection{Error analysis}

\todo{Jiahao: Derive correct residual}


\section{``Applications of overloading''}

\subsection{Generalization to Block Lanczos}

\todo{Andreas}

\subsection{Generalization to quaternions and BigFloats}

\todo{Andreas}

\subsection{Distributed linear algebra}

\paragraph{Overloading 5 functions gets you ScaLAPACK/Elemental integration.}

\todo{Andreas and Jake}



\section{Results}

Pick some specific examples

Pick some convenient Matrix Market matrix which produces some acceleration of
the problem by monitoring the error bounds on individual matrices vs the entire
residual norm.

\missingfigure{Convergence as a function of error bars on singular values vs residuals}

\todo{Jake}


\subsection{Serial linear algebra}

\missingfigure{Execution time of Julia vs ARPACK vs PROPACK vs SLEPc on one core}


\todo{Andreas and Jake and Jiahao}


\subsection{Comparison with randomized algorithms}

Comparison with flashpca or randomized subspace interaction?

Show error bound computations.

\todo{Jiahao}




\subsection{Distributed linear algebra}

\missingfigure{Strong scaling for parallel linalg on some problem}

\missingfigure{Weak scaling for parallel linalg on some problem}

Julia/ScaLAPACK vs Julia/Elemental vs Julia/MKL vs SLEPc vs PARPACK vs Parallel PROPACK (?)


\todo{Andreas and Jake}


\section{Results on genomics data matrix}

\subsection{Spectra of genomics matrices}

\begin{figure}
\includegraphics[width=\textwidth]{fig/scree/fig-scree.pdf}

\caption{Scree plot of the singular values of a genomics data matrix of size $81700\times41505$
(black solid lines), showing the presence of a large, low rank portion
of approximately rank 10 (inset), and an asymptotic convergence to
a random distribution, the Marchenko-Pastur law (red dotted lines),
describing the singular values of a random matrix with rows/columns
ratio $\lambda=0.58$ and iid entries of mean 0 and variance $\sigma^{2}=1.33$.}
\end{figure}

\subsection{Accuracy and speed results}

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
 & Algorithm & mvps & Wall time & $\beta_{final}$ & $\left\Vert \Delta\theta\right\Vert _{1}$ & $\left\Vert \Delta\theta\right\Vert _{\infty}$\tabularnewline
\hline
\hline
FlashPCA1 & Block power & 2280 & 2616.1 sec. & 6.4 & $4\times10^{+4}$ & $4\times10^{+4}$\tabularnewline
\hline
FlashPCA2 & Block power &  &  &  &  & \tabularnewline
\hline
PROPACK & GKL (PRO, NR) & 298 & 264.6 & - & $4\times10^{-8}$ & $4\times10^{-8}$\tabularnewline
\hline
ARPACK & GKL (FRO, IR) & 355 & 858.5 & 7.8 & $6\times10^{-3}$ & $1\times10^{-3}$\tabularnewline
\hline
this work & GKL (FRO, TR) & 280 & 397.8 & 6.4 & $2\times10^{-3}$ & $2\times10^{-3}$\tabularnewline
\hline
this work & GKL (PRO, NR) & 321 & 302.4 & 15072 & $6\times10^{-4}$ & $6\times10^{-4}$\tabularnewline
\hline
\end{tabular}

\caption{Comparing various methods for computing the top 20 principal
components on a simulated genotype matrix of size $80000\times40000$.
Linear algebra kernels were run on OpenBLAS v0.2.18 with 16 software threads.
FlashPCA1 - Reimplementation in pure Julia of the published version of FlashPCA,
FlashPCA2 - Reimplementation in pure Julia of the master version of FlashPCA available on GitHub,
GKL - Golub--Kahan--Lanczos bidiagonalization,
PRO - partial reorthogonalization,
FRO - full reorthogonalization,
NR - no restart,
IR - implicit restart after 40 Lanczos vectors,
TR - thick restart after 40 Lanczos vectors,
mvps - number of matrix--vector products.
Termination criterion set to $\Vert Y\Vert = 10^{-8}$ for FlashPCA$n$,
and relative error in the singular values of $10^{-8}$ for the Lanczos methods.
}
\end{table}



\section{Conclusions}



\section*{Acknowledgments}

We thank the Julia development community for their contributions to free and
open source software. Julia is free software that can be downloaded from
\url{julialang.org/downloads}. The implementation of iterative SVD described in
this paper is available as the \verb|svdl| function in the
\url[IterativeSolvers.jl]{https://github.com/JuliaLang/IterativeSolvers.jl}
package. J.C. would also like to thank Jack Poulson (Stanford), David Silvester
(Manchester), and Alex Townsend (MIT) for many insightful discussions.

\bibliography{svd}

\end{document}
